# ðŸ“–  Multimodal Embedding Papers


### 1. [E5-V: Universal embeddings with multimodal large language models](https://arxiv.org/abs/2407.12580)

### 2. [VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks](https://arxiv.org/pdf/2410.05160)

### 3. [MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs](https://arxiv.org/abs/2411.02571)

### 4. [Bridging Modalities: Improving Universal Multimodal Retrieval by Multimodal Large Language Models](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Bridging_Modalities_Improving_Universal_Multimodal_Retrieval_by_Multimodal_Large_Language_CVPR_2025_paper.pdf)

### 5. [VladVA: Discriminative Fine-tuning of LVLMs](https://arxiv.org/pdf/2412.04378)

### 6. [LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_LamRA_Large_Multimodal_Model_as_Your_Advanced_Retrieval_Assistant_CVPR_2025_paper.pdf)

### 7. [Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval](https://arxiv.org/pdf/2502.11431)

### 8. [LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning](https://arxiv.org/pdf/2503.04812)

### 9. [CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning](https://arxiv.org/abs/2412.14783)

### 10. [UniME: Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs](https://arxiv.org/pdf/2504.17432)

### 11. [Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining](https://arxiv.org/abs/2505.11293)

### 12. [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/pdf/2505.15877)

### 13. [Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval](https://arxiv.org/pdf/2505.19650)

### 14. [Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying](https://arxiv.org/pdf/2506.02020)

### 15. [Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models](https://openaccess.thecvf.com/content/CVPR2025/papers/Cui_Incorporating_Dense_Knowledge_Alignment_into_Unified_Multimodal_Representation_Models_CVPR_2025_paper.pdf)

### 16. [Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment](https://arxiv.org/pdf/2506.06970)

### 17. [MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval](https://arxiv.org/pdf/2506.12364)

### 18. [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)

### 19. [PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal Retrieval with Modality-Adaptive Learning](https://arxiv.org/pdf/2507.08064)

### 20. [UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings](https://arxiv.org/pdf/2505.11815)

### 21. [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/pdf/2507.14902)

### 22. [Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment](https://arxiv.org/pdf/2508.02762)

### 23. [From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model](https://arxiv.org/pdf/2508.00955)

### 24. [WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM](https://arxiv.org/pdf/2509.21990)

### 25. [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/pdf/2510.02745)

### 26. [VIRTUE: Visual-Interactive Text-Image Universal Embedder](https://arxiv.org/pdf/2510.00523)

### 27. [Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents](https://arxiv.org/abs/2507.04590)

### 28. [CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning](https://arxiv.org/pdf/2510.08003)

### 29. [SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model](https://arxiv.org/pdf/2510.12709)

### 30. [Multi-Modal Multi-Task Unified Embedding Model (M3T-UEM): A Task-Adaptive Representation Learning Framework](https://openaccess.thecvf.com/content/ICCV2025/papers/Sharma_Multi-Modal_Multi-Task_Unified_Embedding_Model_M3T-UEM_A_Task-Adaptive_Representation_Learning_ICCV_2025_paper.pdf)

### 31. [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/pdf/2510.17354)

### 32. [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/pdf/2510.05014)

### 33. [MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction](https://arxiv.org/pdf/2509.18095)

### 34. [UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning](https://arxiv.org/pdf/2510.13515)


